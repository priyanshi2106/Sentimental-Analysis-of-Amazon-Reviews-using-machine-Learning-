{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "BERT_sa.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3fQyPUd_NtC",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:29:32.236793Z",
          "iopub.execute_input": "2021-10-25T18:29:32.237374Z",
          "iopub.status.idle": "2021-10-25T18:29:39.971149Z",
          "shell.execute_reply.started": "2021-10-25T18:29:32.237280Z",
          "shell.execute_reply": "2021-10-25T18:29:39.970382Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99604cc3-b7fe-4ebd-c6ca-b6ad08a152ae"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "!pip install transformers\n",
        "import transformers\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYPYQU2qR7aR"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DraHyXk7yIjL"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Yp31eo2_iJq",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:29:48.135039Z",
          "iopub.execute_input": "2021-10-25T18:29:48.135636Z",
          "iopub.status.idle": "2021-10-25T18:29:49.863862Z",
          "shell.execute_reply.started": "2021-10-25T18:29:48.135599Z",
          "shell.execute_reply": "2021-10-25T18:29:49.863115Z"
        },
        "trusted": true
      },
      "source": [
        "from tqdm import tqdm\n",
        "from sklearn import model_selection\n",
        "from sklearn import metrics\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaV9GMA2_sZk",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:29:51.361401Z",
          "iopub.execute_input": "2021-10-25T18:29:51.361857Z",
          "iopub.status.idle": "2021-10-25T18:29:56.192138Z",
          "shell.execute_reply.started": "2021-10-25T18:29:51.361819Z",
          "shell.execute_reply": "2021-10-25T18:29:56.191229Z"
        },
        "trusted": true
      },
      "source": [
        "DEVICE = \"cuda\"\n",
        "MAX_LEN=64\n",
        "TRAIN_BATCH_SIZE=64\n",
        "VALID_BATCH_SIZE=64\n",
        "TEST_BATCH_SIZE=64\n",
        "EPOCHS=2\n",
        "MODEL_PATH = \"./model_BERT.bin\"\n",
        "BERT_PRE_TRAINED_MODEL = \"bert-base-uncased\"\n",
        "TOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PRE_TRAINED_MODEL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-25T18:29:56.194235Z",
          "iopub.execute_input": "2021-10-25T18:29:56.194659Z",
          "iopub.status.idle": "2021-10-25T18:31:37.867649Z",
          "shell.execute_reply.started": "2021-10-25T18:29:56.194618Z",
          "shell.execute_reply": "2021-10-25T18:31:37.866972Z"
        },
        "trusted": true,
        "id": "OMF51YSDyFbL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "outputId": "3578c7a6-e5d9-4b99-ccf7-8bbabc0361d6"
      },
      "source": [
        "train = pd.read_csv('/content/train.csv',engine='python', error_bad_lines=False,names=['label','title','text'])\n",
        "train['review'] = train[['title', 'text']].astype(str).apply(' '.join, axis=1)\n",
        "train.drop('title', axis=1, inplace=True)\n",
        "train.drop('text', axis=1, inplace=True)\n",
        "train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Skipping line 2421411: unexpected end of data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Stuning even for the non-gamer This sound trac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The best soundtrack ever to anything. I'm read...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Amazing! This soundtrack is my favorite music ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>Excellent Soundtrack I truly like this soundtr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421405</th>\n",
              "      <td>1</td>\n",
              "      <td>Scarecrows is a near 'miss' with few scares I ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421406</th>\n",
              "      <td>1</td>\n",
              "      <td>It's not an Uncut Unrated Restore Version, thi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421407</th>\n",
              "      <td>1</td>\n",
              "      <td>haven't watched it I ordered this movie for a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421408</th>\n",
              "      <td>1</td>\n",
              "      <td>Baaaaaaaaaaaaaaaaad Sooooooooooo pointless and...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2421409</th>\n",
              "      <td>1</td>\n",
              "      <td>Let's not sugar coat this review.... I can't b...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2421410 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                             review\n",
              "0            2  Stuning even for the non-gamer This sound trac...\n",
              "1            2  The best soundtrack ever to anything. I'm read...\n",
              "2            2  Amazing! This soundtrack is my favorite music ...\n",
              "3            2  Excellent Soundtrack I truly like this soundtr...\n",
              "4            2  Remember, Pull Your Jaw Off The Floor After He...\n",
              "...        ...                                                ...\n",
              "2421405      1  Scarecrows is a near 'miss' with few scares I ...\n",
              "2421406      1  It's not an Uncut Unrated Restore Version, thi...\n",
              "2421407      1  haven't watched it I ordered this movie for a ...\n",
              "2421408      1  Baaaaaaaaaaaaaaaaad Sooooooooooo pointless and...\n",
              "2421409      1  Let's not sugar coat this review.... I can't b...\n",
              "\n",
              "[2421410 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x1544CHAA-i",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:43:42.196145Z",
          "iopub.execute_input": "2021-10-25T18:43:42.196970Z",
          "iopub.status.idle": "2021-10-25T18:43:53.209417Z",
          "shell.execute_reply.started": "2021-10-25T18:43:42.196932Z",
          "shell.execute_reply": "2021-10-25T18:43:53.208711Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "ee5940cd-5d22-49be-e1e9-6af0af61d41c"
      },
      "source": [
        "test = pd.read_csv('/content/test.csv',engine='python',error_bad_lines=False,names=['label','title','text'])\n",
        "test['review'] = test[['title', 'text']].astype(str).apply(' '.join, axis=1)\n",
        "test.drop('title', axis=1, inplace=True)\n",
        "test.drop('text', axis=1, inplace=True)\n",
        "test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>Great CD My lovely Pat has one of the GREAT vo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>One of the best game music soundtracks - for a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>Batteries died within a year ... I bought this...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>works fine, but Maha Energy is better Check ou...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>Great for the non-audiophile Reviewed quite a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399995</th>\n",
              "      <td>1</td>\n",
              "      <td>Unbelievable- In a Bad Way We bought this Thom...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399996</th>\n",
              "      <td>1</td>\n",
              "      <td>Almost Great, Until it Broke... My son recieve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399997</th>\n",
              "      <td>1</td>\n",
              "      <td>Disappointed !!! I bought this toy for my son ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399998</th>\n",
              "      <td>2</td>\n",
              "      <td>Classic Jessica Mitford This is a compilation ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>399999</th>\n",
              "      <td>1</td>\n",
              "      <td>Comedy Scene, and Not Heard This DVD will be a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>400000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        label                                             review\n",
              "0           2  Great CD My lovely Pat has one of the GREAT vo...\n",
              "1           2  One of the best game music soundtracks - for a...\n",
              "2           1  Batteries died within a year ... I bought this...\n",
              "3           2  works fine, but Maha Energy is better Check ou...\n",
              "4           2  Great for the non-audiophile Reviewed quite a ...\n",
              "...       ...                                                ...\n",
              "399995      1  Unbelievable- In a Bad Way We bought this Thom...\n",
              "399996      1  Almost Great, Until it Broke... My son recieve...\n",
              "399997      1  Disappointed !!! I bought this toy for my son ...\n",
              "399998      2  Classic Jessica Mitford This is a compilation ...\n",
              "399999      1  Comedy Scene, and Not Heard This DVD will be a...\n",
              "\n",
              "[400000 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-25T18:44:41.744206Z",
          "iopub.execute_input": "2021-10-25T18:44:41.744789Z",
          "iopub.status.idle": "2021-10-25T18:44:42.935085Z",
          "shell.execute_reply.started": "2021-10-25T18:44:41.744751Z",
          "shell.execute_reply": "2021-10-25T18:44:42.934355Z"
        },
        "trusted": true,
        "id": "DFHRnyHcyFbM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "b4d8a6f8-2bae-4dd4-e1c4-ce9378f7753b"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, valid = train_test_split(train, test_size=0.15, random_state=42)\n",
        "valid"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2024740</th>\n",
              "      <td>1</td>\n",
              "      <td>These gloves are horrible I bought these for m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1308502</th>\n",
              "      <td>1</td>\n",
              "      <td>Fire in the Heart As this was a gift and I hav...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1763733</th>\n",
              "      <td>1</td>\n",
              "      <td>Why? Sorry, but this just doesn't cut it. The ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>390193</th>\n",
              "      <td>2</td>\n",
              "      <td>My life transformed As an archiecture student,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2216687</th>\n",
              "      <td>1</td>\n",
              "      <td>So Strange I like costume movies and thought t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1457534</th>\n",
              "      <td>1</td>\n",
              "      <td>Rather dull and weak follow-up to the first. T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567153</th>\n",
              "      <td>1</td>\n",
              "      <td>Superficial, hardly any practical value The bo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2277281</th>\n",
              "      <td>1</td>\n",
              "      <td>Flat Flat World A friend recommended this book...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1426653</th>\n",
              "      <td>1</td>\n",
              "      <td>God, I hated this book It's rare that I come a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1460456</th>\n",
              "      <td>2</td>\n",
              "      <td>Oh, The Places You'll Go! I love the book! It'...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>363212 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                             review\n",
              "2024740      1  These gloves are horrible I bought these for m...\n",
              "1308502      1  Fire in the Heart As this was a gift and I hav...\n",
              "1763733      1  Why? Sorry, but this just doesn't cut it. The ...\n",
              "390193       2  My life transformed As an archiecture student,...\n",
              "2216687      1  So Strange I like costume movies and thought t...\n",
              "...        ...                                                ...\n",
              "1457534      1  Rather dull and weak follow-up to the first. T...\n",
              "567153       1  Superficial, hardly any practical value The bo...\n",
              "2277281      1  Flat Flat World A friend recommended this book...\n",
              "1426653      1  God, I hated this book It's rare that I come a...\n",
              "1460456      2  Oh, The Places You'll Go! I love the book! It'...\n",
              "\n",
              "[363212 rows x 2 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-25T18:44:57.061942Z",
          "iopub.execute_input": "2021-10-25T18:44:57.062848Z",
          "iopub.status.idle": "2021-10-25T18:44:57.070282Z",
          "shell.execute_reply.started": "2021-10-25T18:44:57.062805Z",
          "shell.execute_reply": "2021-10-25T18:44:57.069607Z"
        },
        "trusted": true,
        "id": "V7mMeuLGyFbN"
      },
      "source": [
        "test['label'].replace({2: 0}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-25T18:44:59.502850Z",
          "iopub.execute_input": "2021-10-25T18:44:59.503122Z",
          "iopub.status.idle": "2021-10-25T18:44:59.515789Z",
          "shell.execute_reply.started": "2021-10-25T18:44:59.503092Z",
          "shell.execute_reply": "2021-10-25T18:44:59.515100Z"
        },
        "trusted": true,
        "id": "RuzlA_cEyFbN"
      },
      "source": [
        "train['label'].replace({2: 0}, inplace=True)\n",
        "valid['label'].replace({2: 0}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "275scng3AXna",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:45:03.915529Z",
          "iopub.execute_input": "2021-10-25T18:45:03.916086Z",
          "iopub.status.idle": "2021-10-25T18:45:03.923211Z",
          "shell.execute_reply.started": "2021-10-25T18:45:03.916034Z",
          "shell.execute_reply": "2021-10-25T18:45:03.922499Z"
        },
        "trusted": true
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "        self.reviews = reviews\n",
        "        self.targets = targets\n",
        "        self.tokenizer = TOKENIZER\n",
        "        self.max_len = MAX_LEN\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.reviews)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        review = str(self.reviews[item])\n",
        "        target = self.targets[item]\n",
        "        \n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            review,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            pad_to_max_length=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'review_text': review,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'targets': torch.tensor(target, dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JT-i0YtOA6sc",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:45:08.660599Z",
          "iopub.execute_input": "2021-10-25T18:45:08.660883Z",
          "iopub.status.idle": "2021-10-25T18:45:08.670571Z",
          "shell.execute_reply.started": "2021-10-25T18:45:08.660851Z",
          "shell.execute_reply": "2021-10-25T18:45:08.669645Z"
        },
        "trusted": true
      },
      "source": [
        "train_dataset = Dataset(reviews=train.review.values, targets=train.label.values,tokenizer=TOKENIZER,\n",
        "                            max_len=MAX_LEN)\n",
        "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXjQ0hqiB44I",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:45:25.703603Z",
          "iopub.execute_input": "2021-10-25T18:45:25.704216Z",
          "iopub.status.idle": "2021-10-25T18:45:25.715819Z",
          "shell.execute_reply.started": "2021-10-25T18:45:25.704173Z",
          "shell.execute_reply": "2021-10-25T18:45:25.715122Z"
        },
        "trusted": true
      },
      "source": [
        "valid_dataset = Dataset(reviews=valid.review.values, targets=valid.label.values,tokenizer=TOKENIZER,\n",
        "                            max_len=MAX_LEN)\n",
        "valid_data_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, num_workers=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJU-QojvCMFY",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:45:28.417907Z",
          "iopub.execute_input": "2021-10-25T18:45:28.418571Z",
          "iopub.status.idle": "2021-10-25T18:45:28.425838Z",
          "shell.execute_reply.started": "2021-10-25T18:45:28.418527Z",
          "shell.execute_reply": "2021-10-25T18:45:28.424896Z"
        },
        "trusted": true
      },
      "source": [
        "class BERTBaseUncasedClassifier(nn.Module):\n",
        "    def __init__(self,n_classes):\n",
        "        super(BERTBaseUncasedClassifier, self).__init__()\n",
        "        self.bert = transformers.BertModel.from_pretrained(BERT_PRE_TRAINED_MODEL)\n",
        "        self.bert_drop = nn.Dropout(0.3) #Regularization\n",
        "        self.out = nn.Linear(self.bert.config.hidden_size, n_classes) #Fully connected Layer\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        _, pooled_output = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            return_dict=False\n",
        "        )\n",
        "        output = self.bert_drop(pooled_output)\n",
        "        return self.out(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zaYYO9BCOPX",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:45:44.509299Z",
          "iopub.execute_input": "2021-10-25T18:45:44.509958Z",
          "iopub.status.idle": "2021-10-25T18:45:50.880564Z",
          "shell.execute_reply.started": "2021-10-25T18:45:44.509919Z",
          "shell.execute_reply": "2021-10-25T18:45:50.879824Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63508d38-4af3-4e2d-d6f0-74ee8128562b"
      },
      "source": [
        "device = torch.device(DEVICE)\n",
        "model = BERTBaseUncasedClassifier(2)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BERTBaseUncasedClassifier(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (bert_drop): Dropout(p=0.3, inplace=False)\n",
              "  (out): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSL27dSGColh",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:46:01.872644Z",
          "iopub.execute_input": "2021-10-25T18:46:01.873127Z",
          "iopub.status.idle": "2021-10-25T18:46:01.882375Z",
          "shell.execute_reply.started": "2021-10-25T18:46:01.873084Z",
          "shell.execute_reply": "2021-10-25T18:46:01.881492Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9ff9e6-fe7a-4e7f-c463-aad0f286c8b9"
      },
      "source": [
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "print(\"Length of Train Data Loader: \",len(train_data_loader))\n",
        "print(\"Total Steps: \",total_steps)\n",
        "print(\"Epochs: \",EPOCHS)\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer,num_warmup_steps=0,num_training_steps=total_steps)\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of Train Data Loader:  32160\n",
            "Total Steps:  64320\n",
            "Epochs:  2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWG_ewrPT0II",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:46:04.150882Z",
          "iopub.execute_input": "2021-10-25T18:46:04.151643Z",
          "iopub.status.idle": "2021-10-25T18:46:04.157406Z",
          "shell.execute_reply.started": "2021-10-25T18:46:04.151588Z",
          "shell.execute_reply": "2021-10-25T18:46:04.156612Z"
        },
        "trusted": true
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "def f1_score_func(predictedValues, trueValues):\n",
        "    preds = predictedValues.cpu().detach().numpy()\n",
        "    true = trueValues.cpu().flatten()\n",
        "    return f1_score(true, preds, average = 'weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqNqtvzECqvM",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:46:07.126403Z",
          "iopub.execute_input": "2021-10-25T18:46:07.127006Z",
          "iopub.status.idle": "2021-10-25T18:46:07.135579Z",
          "shell.execute_reply.started": "2021-10-25T18:46:07.126965Z",
          "shell.execute_reply": "2021-10-25T18:46:07.134586Z"
        },
        "trusted": true
      },
      "source": [
        "def train_epoch(model,data_loader,loss_fn,optimizer,device,scheduler,n_examples):\n",
        "    model = model.train()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    for bi, d in tqdm(enumerate(data_loader),total=len(data_loader)):\n",
        "        input_ids = d[\"input_ids\"].to(device)\n",
        "        attention_mask = d[\"attention_mask\"].to(device)\n",
        "        targets = d[\"targets\"].to(device)\n",
        "        outputs = model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        \n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "    train_f1 = f1_score_func(preds, targets)\n",
        "\n",
        "    return train_f1, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lp7TJ3kC0Pz",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:46:11.299374Z",
          "iopub.execute_input": "2021-10-25T18:46:11.299680Z",
          "iopub.status.idle": "2021-10-25T18:46:11.312544Z",
          "shell.execute_reply.started": "2021-10-25T18:46:11.299645Z",
          "shell.execute_reply": "2021-10-25T18:46:11.311851Z"
        },
        "trusted": true
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "    model = model.eval()\n",
        "    losses = []\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for bi, d in tqdm(enumerate(data_loader),total=len(data_loader)):\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)\n",
        "            targets = d[\"targets\"].to(device)\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask\n",
        "              )\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    val_f1 = f1_score_func(preds, targets)\n",
        "\n",
        "    return val_f1, np.mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxXJ76-iCCOl",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:46:13.704459Z",
          "iopub.execute_input": "2021-10-25T18:46:13.704971Z",
          "iopub.status.idle": "2021-10-25T18:46:13.709051Z",
          "shell.execute_reply.started": "2021-10-25T18:46:13.704933Z",
          "shell.execute_reply": "2021-10-25T18:46:13.708123Z"
        },
        "trusted": true
      },
      "source": [
        "train_len = len(train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnuQ8tCFD3Ar",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:46:19.008395Z",
          "iopub.execute_input": "2021-10-25T18:46:19.008938Z",
          "iopub.status.idle": "2021-10-25T18:46:19.014507Z",
          "shell.execute_reply.started": "2021-10-25T18:46:19.008901Z",
          "shell.execute_reply": "2021-10-25T18:46:19.013645Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca6afb7b-cf04-41f2-d596-7a3931622785"
      },
      "source": [
        "valid_len = len(valid)\n",
        "valid_len"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "363212"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmfcPKGOC2ig",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:46:22.472744Z",
          "iopub.execute_input": "2021-10-25T18:46:22.473015Z",
          "iopub.status.idle": "2021-10-25T18:46:24.987175Z",
          "shell.execute_reply.started": "2021-10-25T18:46:22.472986Z",
          "shell.execute_reply": "2021-10-25T18:46:24.985968Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12914ac2-c3ca-4118-c9b3-aedaae313fac"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "    train_acc, train_loss = train_epoch(\n",
        "        model,\n",
        "        train_data_loader,\n",
        "        loss_fn,\n",
        "        optimizer,\n",
        "        device,\n",
        "        scheduler,\n",
        "        train_len\n",
        "    )\n",
        "    print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "    val_acc, val_loss = eval_model(\n",
        "    model,\n",
        "    valid_data_loader,\n",
        "    loss_fn,\n",
        "    device,\n",
        "    valid_len\n",
        "    )\n",
        "    print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "    print()\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    \n",
        "    checkpoint = {'model': BERTBaseUncasedClassifier(2),\n",
        "              'state_dict': model.state_dict(),\n",
        "              'optimizer' : optimizer.state_dict()}\n",
        "    \n",
        "    torch.save(checkpoint, 'checkpoint.pth')\n",
        "           \n",
        "    if val_acc > best_accuracy:\n",
        "        #torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "        torch.save(model.state_dict(), MODEL_PATH)\n",
        "        best_accuracy = val_acc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 2542/2542 [51:53<00:00,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.18303093490836356 accuracy 0.9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/449 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 449/449 [03:15<00:00,  2.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.14355945953879432 accuracy 1.0\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/2\n",
            "----------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "  0%|          | 0/2542 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 2542/2542 [51:50<00:00,  1.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss 0.10287387803802157 accuracy 0.9005714285714285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/449 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "100%|██████████| 449/449 [03:16<00:00,  2.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val   loss 0.1587923219000178 accuracy 0.9735914839954006\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6ZhgjTlK05d",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:02:41.85552Z",
          "iopub.status.idle": "2021-10-25T18:02:41.85614Z",
          "shell.execute_reply.started": "2021-10-25T18:02:41.855856Z",
          "shell.execute_reply": "2021-10-25T18:02:41.855883Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "2ada655f-1cb2-4c5e-8f25-632eae2393f6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5udTPKxK4J0",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:02:41.857386Z",
          "iopub.status.idle": "2021-10-25T18:02:41.857957Z",
          "shell.execute_reply.started": "2021-10-25T18:02:41.857706Z",
          "shell.execute_reply": "2021-10-25T18:02:41.857733Z"
        },
        "trusted": true
      },
      "source": [
        "test_dataset = Dataset(reviews=test.review.values, targets=test.label.values,tokenizer=TOKENIZER,\n",
        "                            max_len=MAX_LEN)\n",
        "test_data_loader = torch.utils.data.DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, num_workers=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFSXpcb0LEm5",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:02:41.859087Z",
          "iopub.status.idle": "2021-10-25T18:02:41.859676Z",
          "shell.execute_reply.started": "2021-10-25T18:02:41.859423Z",
          "shell.execute_reply": "2021-10-25T18:02:41.85945Z"
        },
        "trusted": true
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "    model = model.eval()\n",
        "    review_texts = []\n",
        "    predictions = []\n",
        "    prediction_probs = []\n",
        "    real_values = []\n",
        "    with torch.no_grad():\n",
        "        for d in data_loader:\n",
        "            \n",
        "            texts = d[\"review_text\"]\n",
        "            input_ids = d[\"input_ids\"].to(device)\n",
        "            attention_mask = d[\"attention_mask\"].to(device)            \n",
        "            targets = d[\"targets\"].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids,\n",
        "                attention_mask=attention_mask)\n",
        "            \n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "          #  review_texts.extend(texts)\n",
        "            predictions.extend(preds)\n",
        "         #   prediction_probs.extend(outputs)\n",
        "            real_values.extend(targets)\n",
        "        predictions = torch.stack(predictions).cpu()\n",
        "        #prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "        real_values = torch.stack(real_values).cpu()\n",
        "    return predictions, real_values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qaqk5jKLKiy",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:02:41.860852Z",
          "iopub.status.idle": "2021-10-25T18:02:41.861467Z",
          "shell.execute_reply.started": "2021-10-25T18:02:41.861209Z",
          "shell.execute_reply": "2021-10-25T18:02:41.861233Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "235f30c9-1df4-446c-8ebd-18edca617ab3"
      },
      "source": [
        "y_pred, y_test = get_predictions(model,test_data_loader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwkeacaZLbry",
        "execution": {
          "iopub.status.busy": "2021-10-25T18:02:41.862608Z",
          "iopub.status.idle": "2021-10-25T18:02:41.863213Z",
          "shell.execute_reply.started": "2021-10-25T18:02:41.862932Z",
          "shell.execute_reply": "2021-10-25T18:02:41.862964Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de6a370e-c6e1-4b92-a80b-17cffbf552e3"
      },
      "source": [
        "test_f1 = f1_score_func(y_pred, y_test)\n",
        "print(f'Test set F-Score {test_f1}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set F-Score 0.9455566253975067\n"
          ]
        }
      ]
    }
  ]
}